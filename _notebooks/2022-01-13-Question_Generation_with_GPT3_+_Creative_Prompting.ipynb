{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKyaGpSrl2rd"
      },
      "source": [
        "# Question Generation with GPT3 + Creative Prompting\n",
        "\n",
        "> Performing Some Task using GPT-3\n",
        "\n",
        "- toc:true\n",
        "- branch: master\n",
        "- badges: false\n",
        "- image: images/gpt-3.png\n",
        "- comments: true\n",
        "- author: Ritesh Yadav\n",
        "- permalink: /GPT3/\n",
        "- categories: [nlp, gpt3, question generation, prompt engineering]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XZGTMchvJJ1"
      },
      "source": [
        "## Install OpenAI Python SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "0A95wLpYUneF",
        "outputId": "ab239a69-e0ee-4452-c153-9292195d47df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.11.5.tar.gz (466 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 29.4 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 30 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 40 kB 17.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 51 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 61 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 71 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 81 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 92 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 102 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 112 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 122 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 133 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 143 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 153 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 163 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 174 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 184 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 194 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 204 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 215 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 225 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 235 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 245 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 256 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 266 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 276 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 286 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 296 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 307 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 317 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 327 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 337 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 348 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 358 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 368 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 378 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 389 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 399 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 409 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 419 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 430 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 440 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 450 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 460 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 466 kB 11.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.62.3)\n",
            "Collecting pandas>=1.2.3\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 42.7 MB/s \n",
            "\u001b[?25hCollecting pandas-stubs>=1.1.0.11\n",
            "  Downloading pandas_stubs-1.2.0.40-py3-none-any.whl (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 48.3 MB/s \n",
            "\u001b[?25hCollecting openpyxl>=3.0.7\n",
            "  Downloading openpyxl-3.0.9-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[K     |████████████████████████████████| 242 kB 52.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2018.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pandas-stubs>=1.1.0.11->openai) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.11.5-py3-none-any.whl size=161464 sha256=604ae3e818b3dc26bade5925151a4404156543dd2fee70dd437372e651bb86ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/52/66/06c234d9748d81b9622d0b9765c3bd10e99d059ed93f13b0b3\n",
            "Successfully built openai\n",
            "Installing collected packages: pandas-stubs, pandas, openpyxl, openai\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: openpyxl\n",
            "    Found existing installation: openpyxl 2.5.9\n",
            "    Uninstalling openpyxl-2.5.9:\n",
            "      Successfully uninstalled openpyxl-2.5.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.5 which is incompatible.\u001b[0m\n",
            "Successfully installed openai-0.11.5 openpyxl-3.0.9 pandas-1.3.5 pandas-stubs-1.2.0.40\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSYJpXJvYW9G"
      },
      "source": [
        "## Few-shot prompting with question templates\n",
        "-----\n",
        "⚡ **The Idea**\n",
        "\n",
        "For each question type (see below), Create nice reusable question templates with few examples. \n",
        "\n",
        "- Fill in the blanks\n",
        "- MCQs\n",
        "- True or False / Boolean\n",
        "- Multi-choice/Multiple Fill in the blanks\n",
        "\n",
        "Question templates are a combination of prompt tokens and question phrases. For the question generation task \"Few\" in the phrase \"few examples\" depends on the question type. For instance you will witness for Fill in the lanks and MCQs 2 examples works just fine. For True or False you could just go with 1 example and May be in your experiments you may discover Zero-short can be put to best use for your needs.\n",
        "\n",
        "**Note:**\n",
        "- Here examples simply mean string templates of some static knowledge put together to match the desired Question (and answer) style.\n",
        "- Having categories of templates can help, because the closer your templates are with the actual paragraphs you are using to generate question in terms of domains the stronger will be the generation results.\n",
        "\n",
        "**Disclaimer:**\n",
        "- The strength and quality of your generations are a function of your Prompt Tokens and Example phrases. Choose creative yet relevant prompt tokens.\n",
        "- PROMPT PROGRAMMING is a SKILL you can improve purely by TRIAL AND ERROR with your ENGLISH knowledge. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qR9Wdv1cG_h"
      },
      "source": [
        "#### 1. Fill in the blanks\n",
        "------\n",
        "\n",
        "**Below is a sample prompt template for Fill in the blank questions** As said before the examples are based on the some static knowledge. Here we have 2 ``` <Paragraph/Ques-Ans> ``` pairs. You can have as many example as you want. In this example ```Paragraph:, Question:, Answer: ``` are the 3 prompt tokens. \n",
        "\n",
        "* Line 1. Paragraph: Jesus, according to some biblical sources, was born in this town some two millennia ago in Bethlehem. \n",
        "* Line 2. Question: Where was Jesus born ______ ? Answer: Bethlehem \n",
        "* Line 3. Paragraph: Sachin Tendulkar was born in India. He debuted for Indian cricket in 1988. \n",
        "* Line 4. Question: In ______ Sachin started playing cricket? Answer: 1988"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WeKxlflpgCBs"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = \"<your_openai_api_key>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiTjmQzVTfm1",
        "outputId": "30c943fc-0844-48c7-8102-00f8c736a8ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Question..\n",
            "Question: In ______ Elon Musk joined Tesla.\n",
            "Answer: 2004\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt_template= \"\"\"\n",
        "Paragraph: Jesus, according to some biblical sources, was born in this town some two millennia ago in Bethlehem. \n",
        "Question: Where was Jesus born ______ ? Answer: Bethlehem\n",
        "Paragraph: Sachin Tendulkar was born in India. He debuted for Indian cricket in 1988. \n",
        "Question: In ______ Sachin started playing cricket. Answer: 1988\n",
        "\"\"\"\n",
        "\n",
        "custom_prompt=\"\"\"\n",
        "Paragraph: Elon Musk was a born in South Africa in 1971 and he joined Tesla in 2004.\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template + custom_prompt\n",
        "completion = openai.Completion.create(engine=\"davinci\", prompt=prompt, max_tokens=32, temperature=0.7)\n",
        "\n",
        "print(\"Generated Question..\")\n",
        "generated = completion.choices[0].text\n",
        "if \"Paragraph\" in generated:\n",
        "   ind = generated.index(\"Paragraph\") \n",
        "   print(generated[:ind])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8at1fVvdXVh"
      },
      "source": [
        "#### 2. MCQ - Multiple Choice Questions\n",
        "------\n",
        "\n",
        "**Below is a sample prompt template for MCQs** As said before the examples are based on the some static knowledge. Here we have 2 ``` <Paragraph/Ques-Choice-Ans> ``` pairs. You can have as many as you want to make the generation strong.\n",
        "\n",
        "* Line 1. Paragraph: Jesus, according to some biblical sources, was born in this town some two millennia ago in Bethlehem. The story begins with wise men who come to the city of Jerusalem after seeing a star that they interpreted as signaling the birth of a new king.\n",
        "* Line 2. Question: Where was Jesus born ? A) Jerusalem, B) Palestine, C) Bethlehem. D) Tel-Aviv Answer: C\n",
        "* Line 3. Paragraph: Sachin Tendulkar was born in India 1972. He debuted for Indian cricket in 1988 and retired in 2011.\n",
        "* Line 4. Question: In what year Sachin started playing cricket? A) 1972, B) 1988, C) 2011, D) 2001. Answer: B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2Z533gJU1lP",
        "outputId": "0f7a00bc-1286-4f63-f7d6-06dd7e46de15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Question..\n",
            "Question: In what year did Elon join Tesla? A) 1971, B) 2004, C) 1992, D) 2012. Answer: B\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt_template= \"\"\"\n",
        "Paragraph: Jesus, according to some biblical sources, was born in this town some two millennia ago in Bethlehem. The story begins with wise men who come to the city of Jerusalem after seeing a star that they interpreted as signaling the birth of a new king.\n",
        "Question: Where was Jesus born ? A) Jerusalem, B) Palestine, C) Bethlehem. D) Tel-Aviv Answer: C\n",
        "Paragraph: Sachin Tendulkar was born in India 1972. He debuted for Indian cricket in 1988 and retired in 2011.\n",
        "Question: In what year Sachin started playing cricket? A) 1972, B) 1988, C) 2011, D) 2001. Answer: B\n",
        "\"\"\"\n",
        "\n",
        "custom_prompt=\"\"\"\n",
        "Paragraph: Elon Musk was a born in South Africa in 1971 and he joined Tesla in 2004.\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template + custom_prompt\n",
        "completion = openai.Completion.create(engine=\"davinci\", prompt=prompt, max_tokens=32, temperature=0.7)\n",
        "\n",
        "print(\"Generated Question..\")\n",
        "generated = completion.choices[0].text\n",
        "if \"Paragraph\" in generated:\n",
        "   ind = generated.index(\"Paragraph\") \n",
        "   print(generated[:ind])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uso4VLJgG-X"
      },
      "source": [
        "#### 3. True or False / Boolean Questions\n",
        "------\n",
        "Here we have ONLY 1 ``` <Paragraph/Ques-Ans> ``` pair.  But be warned generation might suffer. More the examples in your template Stronger the generations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OQnU3IVexWK",
        "outputId": "d3655291-6743-4ef5-aeea-21397b022afc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Question..\n",
            "Question: Elon Musk was born in Cape Town.\n",
            "Answer: True\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt_template= \"\"\"\n",
        "Paragraph: Jesus, according to some biblical sources, was born in this town some two millennia ago in Bethlehem. The story begins with wise men who come to the city of Jerusalem after seeing a star that they interpreted as signaling the birth of a new king.\n",
        "Question: Jesus was born in Jerusalem. Answer: False\n",
        "\"\"\"\n",
        "\n",
        "custom_prompt=\"\"\"\n",
        "Paragraph: Elon Musk was a born in South Africa in 1971 and he joined Tesla in 2004.\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template + custom_prompt\n",
        "completion = openai.Completion.create(engine=\"davinci\", prompt=prompt, max_tokens=32, temperature=0.7)\n",
        "\n",
        "print(\"Generated Question..\")\n",
        "generated = completion.choices[0].text\n",
        "if \"Paragraph\" in generated:\n",
        "   ind = generated.index(\"Paragraph\") \n",
        "   print(generated[:ind])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y-cAhmsmlL5"
      },
      "source": [
        "#### 4. (Just for fun) Lets try generating higher-order probing questions which either needs common sense or deeper inference.\n",
        "------\n",
        "Note how we changed the prompt strings to ```<fact/probe-inference>``` pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv2DgR0xm3zR",
        "outputId": "6bf88677-cb5c-4cfc-cc12-15d1738f30e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Probe..\n",
            "Probe: What is Musk's current age? Inference: 41 years.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt_template= \"\"\"\n",
        "Fact: Jesus, according to some biblical sources, was born in this town some two millennia ago in Bethlehem. The story begins with wise men who come to the city of Jerusalem after seeing a star that they interpreted as signaling the birth of a new king.\n",
        "Probe: Is Jesus a human being? Inference: No he is a God.\n",
        "Fact: Sachin Tendulkar was born in India 1972. He debuted for Indian cricket in 1988 and retired in 2011.\n",
        "Probe: How many years Sachin played cricket? Inference: 23 years.\n",
        "\"\"\"\n",
        "\n",
        "custom_prompt=\"\"\"\n",
        "Fact: Elon Musk started Tesla Motors since 2004. He is the CEO and product architect of Tesla and he is also the Chairman of Musk Foundation, an organization supporting research on renewable energy, human space exploration and pediatrics. At age 12, sold his code for a video game called “Blastar” to a computer magazine for $500.\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template + custom_prompt\n",
        "completion = openai.Completion.create(engine=\"davinci\", prompt=prompt, max_tokens=32, temperature=0.7)\n",
        "\n",
        "print(\"Generated Probe..\")\n",
        "generated = completion.choices[0].text\n",
        "if \"Fact\" in generated:\n",
        "   ind = generated.index(\"Fact\") \n",
        "   print(generated[:ind])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynx2qOSDgJVO"
      },
      "source": [
        "### Important Note\n",
        "\n",
        "- Signup for OpenAI to get your own API key !\n",
        "- Engine choice: Davinci is not the only engine of choice \n",
        "```\n",
        "# list engines\n",
        "engines = openai.Engine.list()\n",
        "```\n",
        "Play with the list using the above snippet and choose the one that best suits your case.\n",
        "- Number of generations - As you can see, I have limited my generations to only 1. You could iterate over many generations with single prompt\n",
        "- Play with misc Parameters like ```max_tokens=32, temperature=0.7``` Refer this link  -https://beta.openai.com/docs/api-reference/completions/create\n",
        "- Feel free to use your creativity and rxpand to other tasks"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Question Generation with GPT3 + Creative Prompting.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
